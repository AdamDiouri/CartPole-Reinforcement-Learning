{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CartPole.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CartPole Reinforcement Learning Project"
      ],
      "metadata": {
        "id": "CtgHpMlFGNWR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMwR4X5cHZWv"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras import Model\n",
        "import tensorflow_probability as tfp\n",
        "import gym\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MakeModel(Model):\n",
        "  def __init__(self, num_actions):\n",
        "    super().__init__()\n",
        "    self.fc1 = tf.keras.layers.Dense(64, activation='relu')\n",
        "    self.fc2 = tf.keras.layers.Dense(64, activation='relu')\n",
        "    self.fc3 = tf.keras.layers.Dense(64, activation='relu')\n",
        "    self.action = tf.keras.layers.Dense(num_actions, activation='softmax')\n",
        "\n",
        "  def call(self, state):\n",
        "    x = tf.convert_to_tensor(state)\n",
        "    x = self.fc1(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.fc3(x)\n",
        "    x = self.action(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "v9MG-PFkJEND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "  def __init__(self, gamma=0.05, lr=0.001, n_actions=2):\n",
        "    self.gamma = gamma  # Discounting factor for each future reward\n",
        "    self.lr = lr\n",
        "    self.model = MakeModel(n_actions)\n",
        "    self.opt = tf.keras.optimizers.Adam(learning_rate=self.lr)\n",
        "    self.action_memory = []  # Store actions\n",
        "    self.reward_memory = []  # Store rewards\n",
        "    self.state_memory = []  # Store states\n",
        "\n",
        "  def choose_action(self, state):\n",
        "    # This function uses the state to predict an output form the model\n",
        "    # The output will be an array of size (1, number_actions)\n",
        "    # Later on, it will be converted to a probability distribution\n",
        "    # This distribution will be used to select an action based on the probabilities\n",
        "    # Finally, this action will be stored into the action memory we created earlier once the episode ends\n",
        "    prob = self.model(np.array([state]))\n",
        "    dist = tfp.distributions.Categorical(probs=prob, dtype=tf.float32)\n",
        "    action = dist.sample()\n",
        "    self.action_memory.append(action)\n",
        "    return int(action.numpy()[0])\n",
        "\n",
        "  def learn(self):\n",
        "    # This is the main part of the agent class\n",
        "    # This function will tell the model how to learn from the actions and rewards taken in each episode\n",
        "    # First, we calculate the discounted reward\n",
        "    # The discount reward essentially determines how much the agents care about rewards in the distant future relative to those in the immediate future\n",
        "    # Since it's assumed that rewards that are recieved in the recent future carry more importance than that of the future rewards\n",
        "    # The discounted reward formula is: G(t) = R(t+1) + γ*R(t+2) + γ^2*R(t+3)\n",
        "    # Next, we calculate the gradients and the loss for the model training and then optimize them using Adam\n",
        "    sum_reward = 0\n",
        "    discnt_rewards = []\n",
        "    self.reward_memory.reverse()\n",
        "    for r in self.reward_memory:\n",
        "      sum_reward = r + self.gamma*sum_reward\n",
        "      discnt_rewards.append(sum_reward)\n",
        "    discnt_rewards.reverse()\n",
        "\n",
        "    for state, action, reward in zip(self.state_memory, self.action_memory, discnt_rewards):\n",
        "      with tf.GradientTape() as tape:\n",
        "        p = self.model(np.array([state]), training=True)\n",
        "        loss = self.calc_loss(p, action, reward)\n",
        "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
        "        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
        "\n",
        "    self.reward_memory = []\n",
        "    self.action_memory = []\n",
        "    self.state_memory = []\n",
        "\n",
        "  def calc_loss(self, prob, action, reward):\n",
        "    # First, we take the probability distribution of the model output\n",
        "    # Then the log probability is taken form this distribution\n",
        "    # Finally, the reward recieved is multiplied with this probability \n",
        "    dist = tfp.distributions.Categorical(probs=prob, dtype=tf.float32)\n",
        "    log_prob = dist.log_prob(action)\n",
        "    loss = -log_prob*reward\n",
        "    return loss\n",
        "\n",
        "  def store_reward(self, reward):\n",
        "    self.reward_memory.append(reward)\n",
        "\n",
        "  def store_state(self, state):\n",
        "    self.state_memory.append(state)"
      ],
      "metadata": {
        "id": "WY1T4FZPJ1KI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "agent = Agent()\n",
        "num_episodes = 10000"
      ],
      "metadata": {
        "id": "_X81j5khNvFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "LOG_DIR = '/tmp/log'\n",
        "%tensorboard --"
      ],
      "metadata": {
        "id": "osFr8FExdgZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(num_episodes):\n",
        "  state = env.reset()\n",
        "  score = 0\n",
        "  rewards = []\n",
        "  states = []\n",
        "  actions = []\n",
        "  done = False\n",
        "  while not done:\n",
        "    action = agent.choose_action(state=state)\n",
        "    state_, reward, done, _ = env.step(action)\n",
        "    agent.store_reward(reward)\n",
        "    agent.store_state(state)\n",
        "    state = state_\n",
        "    score += reward\n",
        "    # Remove comment to render the GUI\n",
        "    # env.render()\n",
        "    if done:\n",
        "      agent.learn()\n",
        "      print(f'Episode done: {i+1}\\t|\\t Score recieved: {score}')"
      ],
      "metadata": {
        "id": "Uke768QIPRlG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}